"""
VIPER - Public Exploit Search Client

This module searches for publicly available exploits for CVEs from:
1. Exploit-DB
2. GitHub Repositories
"""
import json
import logging
import os
import sys
import time
from datetime import datetime
from typing import Any, Dict, List, Optional, Union

import requests
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_exponential

# Add the project root directory to the path to import modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.utils.config import (
    get_exploit_db_api_url,
    get_exploit_search_max_results,
    get_github_api_url,
    get_github_token,
)

# Set up logging
logger = logging.getLogger(__name__)

# Simple in-memory cache for exploit search results (TTL: 1 hour)
_exploit_cache = {}
_cache_ttl = 3600  # seconds


def _get_from_cache(cve_id: str) -> Optional[List[Dict[str, Any]]]:
    """Get exploit data from cache if it exists and is fresh"""
    if cve_id in _exploit_cache:
        timestamp, data = _exploit_cache[cve_id]
        if time.time() - timestamp < _cache_ttl:
            logger.debug(f"Cache hit for {cve_id}")
            return data
    return None


def _add_to_cache(cve_id: str, data: List[Dict[str, Any]]) -> None:
    """Add exploit data to cache with current timestamp"""
    _exploit_cache[cve_id] = (time.time(), data)
    logger.debug(f"Added {cve_id} to cache with {len(data)} exploits")


@retry(
    retry=retry_if_exception_type((requests.RequestException, ConnectionError, json.JSONDecodeError)),
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=30),
)
def search_exploit_db(cve_id: str) -> List[Dict[str, Any]]:
    """
    Search Exploit-DB for exploits related to a CVE

    Args:
        cve_id: The CVE ID to search for

    Returns:
        List of dictionaries with exploit information
    """
    exploits = []
    api_url = get_exploit_db_api_url()

    try:
        # Exploit-DB's API endpoint with CVE filter
        url = f"{api_url}/search"
        params = {"cve": cve_id.replace("CVE-", "")}

        logger.info(f"Searching Exploit-DB for {cve_id}")
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()

        data = response.json()

        if "data" in data and isinstance(data["data"], list):
            for exploit in data["data"]:
                # Extract relevant fields
                exploit_info = {
                    "source": "Exploit-DB",
                    "title": exploit.get("title", "Unknown Title"),
                    "url": f"https://www.exploit-db.com/exploits/{exploit.get('id', 'unknown')}",
                    "type": exploit.get("type", "Unknown"),
                    "date_published": exploit.get("date_published", "Unknown"),
                    "exploit_id": exploit.get("id"),
                }
                exploits.append(exploit_info)

        logger.info(f"Found {len(exploits)} exploits on Exploit-DB for {cve_id}")

    except requests.RequestException as e:
        logger.error(f"Error searching Exploit-DB: {str(e)}")
        # Re-raise for retry mechanism
        raise
    except json.JSONDecodeError as e:
        logger.error(f"Error parsing Exploit-DB response: {str(e)}")
        raise
    except Exception as e:
        logger.error(f"Unexpected error searching Exploit-DB: {str(e)}")

    return exploits


@retry(
    retry=retry_if_exception_type((requests.RequestException, ConnectionError, json.JSONDecodeError)),
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=30),
)
def search_github(cve_id: str) -> List[Dict[str, Any]]:
    """
    Search GitHub for exploits related to a CVE

    Args:
        cve_id: The CVE ID to search for

    Returns:
        List of dictionaries with exploit information
    """
    exploits = []
    github_token = get_github_token()
    api_url = get_github_api_url()
    max_results = get_exploit_search_max_results()

    if not github_token:
        logger.warning("GitHub token not found. Skipping GitHub search.")
        return exploits

    try:
        # Search for repositories with exploit code for the CVE
        # We use multiple search terms to improve results
        search_queries = [f"{cve_id} exploit", f"{cve_id} PoC", f"{cve_id} proof of concept"]

        headers = {
            "Accept": "application/vnd.github.v3+json",
            "Authorization": f"token {github_token}",
        }

        for query in search_queries:
            logger.info(f"Searching GitHub for: {query}")

            # Search repositories
            repo_url = f"{api_url}/search/repositories"
            repo_params = {"q": query, "sort": "updated", "order": "desc", "per_page": max_results}

            repo_response = requests.get(repo_url, headers=headers, params=repo_params, timeout=10)
            repo_response.raise_for_status()
            repo_data = repo_response.json()

            for repo in repo_data.get("items", []):
                # Check if this is likely an exploit/PoC repository
                name = repo.get("name", "").lower()
                description = repo.get("description", "").lower() if repo.get("description") else ""

                # Look for keywords suggesting this is an exploit repo
                is_exploit = any(
                    keyword in name or keyword in description
                    for keyword in ["exploit", "poc", "proof", "vulnerability", "cve-"]
                )

                if is_exploit:
                    exploit_info = {
                        "source": "GitHub",
                        "title": repo.get("name", "Unknown Repository"),
                        "url": repo.get("html_url", ""),
                        "type": "Repository",
                        "date_published": repo.get("created_at", "Unknown"),
                        "description": repo.get("description", "No description"),
                        "stars": repo.get("stargazers_count", 0),
                    }

                    # Only add if we don't already have this URL
                    if not any(e["url"] == exploit_info["url"] for e in exploits):
                        exploits.append(exploit_info)

            # Avoid rate limiting
            time.sleep(1)

            # Also search code
            code_url = f"{api_url}/search/code"
            code_params = {"q": query, "sort": "indexed", "order": "desc", "per_page": max_results}

            code_response = requests.get(code_url, headers=headers, params=code_params, timeout=10)
            code_response.raise_for_status()
            code_data = code_response.json()

            for code_item in code_data.get("items", []):
                # Extract code information
                repo_name = code_item.get("repository", {}).get("full_name", "Unknown")
                path = code_item.get("path", "Unknown")

                exploit_info = {
                    "source": "GitHub",
                    "title": f"{repo_name}: {path}",
                    "url": code_item.get("html_url", ""),
                    "type": "Code",
                    "date_published": "Unknown",  # GitHub API doesn't provide creation date for code search results
                }

                # Only add if we don't already have this URL
                if not any(e["url"] == exploit_info["url"] for e in exploits):
                    exploits.append(exploit_info)

            # Avoid rate limiting
            time.sleep(1)

        logger.info(f"Found {len(exploits)} potential exploits on GitHub for {cve_id}")

    except requests.RequestException as e:
        logger.error(f"Error searching GitHub: {str(e)}")
        raise
    except json.JSONDecodeError as e:
        logger.error(f"Error parsing GitHub response: {str(e)}")
        raise
    except Exception as e:
        logger.error(f"Unexpected error searching GitHub: {str(e)}")

    return exploits


def find_public_exploits(cve_id: str) -> Optional[List[Dict[str, Any]]]:
    """
    Search for publicly available exploits for a given CVE ID with preference-based fallback

    Priority order:
    1. If EXPLOIT_DB_API_URL is configured, try Exploit-DB first
    2. If Exploit-DB fails or is not configured, try GitHub
    3. Use the first successful source, don't combine results

    Args:
        cve_id: The CVE ID to search for

    Returns:
        List of dictionaries containing exploit information, or None if error occurs
    """
    if not cve_id or not cve_id.startswith("CVE-"):
        logger.error(f"Invalid CVE ID: {cve_id}")
        return None

    # Check cache first
    cached_result = _get_from_cache(cve_id)
    if cached_result is not None:
        return cached_result

    try:
        all_exploits = []

        # Get configuration for exploit search sources
        exploit_db_url = get_exploit_db_api_url()
        github_token = get_github_token()

        # If no sources are configured, log a warning and return empty
        if not exploit_db_url and not github_token:
            logger.warning(
                f"No exploit search sources configured for {cve_id}. Configure EXPLOIT_DB_API_URL or GITHUB_TOKEN."
            )
            return []

        # Priority 1: Try Exploit-DB first if configured
        if exploit_db_url:
            logger.info(f"Searching Exploit-DB for {cve_id} (primary source)")
            try:
                exploit_db_results = search_exploit_db(cve_id)
                if exploit_db_results:
                    logger.info(f"Found {len(exploit_db_results)} exploits on Exploit-DB for {cve_id}")
                    all_exploits.extend(exploit_db_results)

                    # Sort and cache successful results
                    all_exploits.sort(key=_get_sort_key, reverse=True)
                    _add_to_cache(cve_id, all_exploits)
                    return all_exploits
                else:
                    logger.info(f"No exploits found on Exploit-DB for {cve_id}, trying GitHub fallback")
            except Exception as e:
                logger.error(f"Exploit-DB search failed for {cve_id}: {str(e)}, trying GitHub fallback")

        # Priority 2: Try GitHub if Exploit-DB failed or is not configured
        if github_token:
            source_type = "fallback source" if exploit_db_url else "primary source"
            logger.info(f"Searching GitHub for {cve_id} ({source_type})")
            try:
                github_results = search_github(cve_id)
                if github_results:
                    logger.info(f"Found {len(github_results)} exploits on GitHub for {cve_id}")
                    all_exploits.extend(github_results)

                    # Sort and cache successful results
                    all_exploits.sort(key=_get_sort_key, reverse=True)
                    _add_to_cache(cve_id, all_exploits)
                    return all_exploits
                else:
                    logger.info(f"No exploits found on GitHub for {cve_id}")
            except Exception as e:
                logger.error(f"GitHub search failed for {cve_id}: {str(e)}")

        # If we get here, either no sources worked or no exploits were found
        logger.info(f"No exploits found for {cve_id} from any configured source")

        # Cache empty result to avoid repeated failed searches
        _add_to_cache(cve_id, all_exploits)
        return all_exploits

    except Exception as e:
        logger.error(f"Error finding public exploits for {cve_id}: {str(e)}")
        return None


def _get_sort_key(exploit):
    """
    Helper function to extract sort key from exploit for date sorting
    """
    date_published = exploit.get("date_published", "Unknown")
    if isinstance(date_published, str) and date_published != "Unknown":
        try:
            # Remove timezone information if present to ensure all naive datetimes
            if "Z" in date_published:
                # Convert to naive datetime
                return datetime.fromisoformat(date_published.replace("Z", ""))
            elif "+" in date_published or "-" in date_published and "T" in date_published:
                # For dates with timezone info, convert to naive datetime
                dt = datetime.fromisoformat(date_published)
                return dt.replace(tzinfo=None)
            else:
                # Already a naive datetime string
                return datetime.fromisoformat(date_published)
        except (ValueError, TypeError):
            # If we can't parse it, use minimum datetime
            return datetime.min
    return datetime.min
